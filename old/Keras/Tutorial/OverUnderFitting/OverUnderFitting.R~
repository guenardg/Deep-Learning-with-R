##
### Tutorial: Overfitting and Underfitting
##
## In two of the previous tutorials — classifying movie reviews, and
## predicting housing prices — we saw that the accuracy of our model
## on the validation data would peak after training for a number of
## epochs, and would then start decreasing.
##
## In other words, our model would overfit to the training
## data. Learning how to deal with overfitting is important. Although
## it’s often possible to achieve high accuracy on the training set,
## what we really want is to develop models that generalize well to
## testing data (or data they haven’t seen before).
##
## The opposite of overfitting is underfitting. Underfitting occurs
## when there is still room for improvement on the test data. This can
## happen for a number of reasons: If the model is not powerful
## enough, is over-regularized, or has simply not been trained long
## enough. This means the network has not learned the relevant
## patterns in the training data.
##
## If you train for too long though, the model will start to overfit
## and learn patterns from the training data that don’t generalize to
## the test data. We need to strike a balance. Understanding how to
## train for an appropriate number of epochs as we’ll explore below is
## a useful skill.
##
## To prevent overfitting, the best solution is to use more training
## data. A model trained on more data will naturally generalize
## better. When that is no longer possible, the next best solution is
## to use techniques like regularization. These place constraints on
## the quantity and type of information your model can store. If a
## network can only afford to memorize a small number of patterns, the
## optimization process will force it to focus on the most prominent
## patterns, which have a better chance of generalizing well.
##
## In this tutorial, we’ll explore two common regularization
## techniques — weight regularization and dropout — and use them to
## improve our IMDB movie review classification results.
##
library(keras)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tibble)
##
## Download the IMDB dataset
num_words <- 10000
load(file="../Data/imdb.rda")
##
c(train_data, train_labels) %<-% imdb$train
c(test_data, test_labels) %<-% imdb$test
##
## Rather than using an embedding as in the previous notebook, here we
## will multi-hot encode the sentences. This model will quickly
## overfit to the training set. It will be used to demonstrate when
## overfitting occurs, and how to fight it.
##
## Multi-hot-encoding our lists means turning them into vectors of 0s
## and 1s. Concretely, this would mean for instance turning the
## sequence [3, 5] into a 10,000-dimensional vector that would be
## all-zeros except for indices 3 and 5, which would be ones.
##
multi_hot_sequences <- function(sequences, dimension) {
    multi_hot <- matrix(0, nrow = length(sequences), ncol = dimension)
    for (i in 1L:length(sequences)) {
        multi_hot[i, sequences[[i]]] <- 1
    }
    multi_hot
}
##
train_data <- multi_hot_sequences(train_data, num_words)
test_data <- multi_hot_sequences(test_data, num_words)
##
## Let’s look at one of the resulting multi-hot vectors. The word
## indices are sorted by frequency, so it is expected that there are
## more 1-values near index zero, as we can see in this plot:
first_text <- data.frame(word = 1L:10000L, value = train_data[1L,])
ggplot(first_text, aes(x = word, y = value)) +
    geom_line() +
    theme(axis.title.y = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank())
##
## Demonstrate overfitting
##
## The simplest way to prevent overfitting is to reduce the size of
## the model, i.e. the number of learnable parameters in the model
## (which is determined by the number of layers and the number of
## units per layer). In deep learning, the number of learnable
## parameters in a model is often referred to as the model’s
## “capacity”. Intuitively, a model with more parameters will have
## more “memorization capacity” and therefore will be able to easily
## learn a perfect dictionary-like mapping between training samples
## and their targets, a mapping without any generalization power, but
## this would be useless when making predictions on previously unseen
## data.
##
## Always keep this in mind: deep learning models tend to be good at
## fitting to the training data, but the real challenge is
## generalization, not fitting.
##
## On the other hand, if the network has limited memorization
## resources, it will not be able to learn the mapping as easily. To
## minimize its loss, it will have to learn compressed representations
## that have more predictive power. At the same time, if you make your
## model too small, it will have difficulty fitting to the training
## data. There is a balance between “too much capacity” and “not
## enough capacity”.
##
## Unfortunately, there is no magical formula to determine the right
## size or architecture of your model (in terms of the number of
## layers, or what the right size for each layer). You will have to
## experiment using a series of different architectures.
##
## To find an appropriate model size, it’s best to start with
## relatively few layers and parameters, then begin increasing the
## size of the layers or adding new layers until you see diminishing
## returns on the validation loss. Let’s try this on our movie review
## classification network.
##
## We’ll create a simple model using only dense layers, then well a
## smaller version, and compare them.
##
### Create a baseline model
baseline_model <- 
    keras_model_sequential() %>%
    layer_dense(units = 16, activation = "relu", input_shape = 10000) %>%
    layer_dense(units = 16, activation = "relu") %>%
    layer_dense(units = 1, activation = "sigmoid")
##
baseline_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = list("accuracy")
)
##
baseline_model %>% summary()
##







































